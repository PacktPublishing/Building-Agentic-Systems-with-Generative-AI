{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ef500a",
   "metadata": {},
   "source": [
    "# Chapter 4 – Reflection and Introspection in Agents\n",
    "---\n",
    "\n",
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6158ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai ipywidgets crewai pysqlite3-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478113ec",
   "metadata": {},
   "source": [
    "# 1. Meta Reasoning - example\n",
    "---\n",
    "\n",
    "Let's take a look at a simple meta-reasoning approach without AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac6d01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee2139",
   "metadata": {},
   "source": [
    "## Simulated travel agent with meta-reasoning capabilities\n",
    "\n",
    "- recommend_destination: The agent recommends a destination based on user preferences (budget, luxury, adventure) and internal weightings.\n",
    "\n",
    "- get_user_feedback: The agent receives feedback on the recommendation (positive or negative).\n",
    "\n",
    "- meta_reasoning: The agent adjusts its reasoning by updating the weights based on feedback, improving future recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa6edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated travel agent with meta-reasoning capabilities\n",
    "class ReflectiveTravelAgent:\n",
    "    def __init__(self):\n",
    "        # Initialize preference weights that determine how user preferences influence recommendations\n",
    "        self.preferences_weights = {\n",
    "            \"budget\": 0.5,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.3,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.2  # Weight for adventure-related preferences\n",
    "        }\n",
    "        self.user_feedback = []  # List to store user feedback for meta-reasoning\n",
    "\n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"\n",
    "        Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "        Args:\n",
    "            user_preferences (dict): User's preferences with keys like 'budget', 'luxury', 'adventure'\n",
    "\n",
    "        Returns:\n",
    "            str: Recommended destination\n",
    "        \"\"\"\n",
    "        # Calculate scores for each destination based on weighted user preferences\n",
    "        score = {\n",
    "            \"Paris\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] + \n",
    "                      self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"Bangkok\": (self.preferences_weights[\"budget\"] * user_preferences[\"budget\"] +\n",
    "                        self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"New York\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "                         self.preferences_weights[\"budget\"] * user_preferences[\"budget\"])\n",
    "        }\n",
    "        # Select the destination with the highest calculated score\n",
    "        recommendation = max(score, key=score.get)\n",
    "        return recommendation\n",
    "\n",
    "    def get_user_feedback(self, actual_experience):\n",
    "        \"\"\"\n",
    "        Simulate receiving user feedback and trigger meta-reasoning to adjust recommendations.\n",
    "\n",
    "        Args:\n",
    "            actual_experience (str): The destination the user experienced\n",
    "        \"\"\"\n",
    "        # Simulate user feedback: 1 for positive, -1 for negative\n",
    "        feedback = random.choice([1, -1])\n",
    "        print(f\"Feedback for {actual_experience}: {'Positive' if feedback == 1 else 'Negative'}\")\n",
    "        \n",
    "        # Store the feedback for later analysis\n",
    "        self.user_feedback.append((actual_experience, feedback))\n",
    "        \n",
    "        # Trigger meta-reasoning to adjust the agent's reasoning process based on feedback\n",
    "        self.meta_reasoning()\n",
    "\n",
    "    def meta_reasoning(self):\n",
    "        \"\"\"\n",
    "        Analyze collected feedback and adjust preference weights to improve future recommendations.\n",
    "        This simulates the agent reflecting on its reasoning process and making adjustments.\n",
    "        \"\"\"\n",
    "        for destination, feedback in self.user_feedback:\n",
    "            if feedback == -1:  # Negative feedback indicates dissatisfaction\n",
    "                # Reduce the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 0.9  # Decrease luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "            elif feedback == 1:  # Positive feedback indicates satisfaction\n",
    "                # Increase the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 1.1  # Increase luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "\n",
    "        # Normalize weights to ensure they sum up to 1 for consistency\n",
    "        total_weight = sum(self.preferences_weights.values())\n",
    "        for key in self.preferences_weights:\n",
    "            self.preferences_weights[key] /= total_weight\n",
    "\n",
    "        # Display updated weights after meta-reasoning adjustments\n",
    "        print(f\"Updated weights: {self.preferences_weights}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ad2f0",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "- User Preferences: Defines the user's preferences for budget, luxury, and adventure.\n",
    "\n",
    "- First Recommendation: The agent recommends a destination based on the initial weights and user preferences.\n",
    "\n",
    "- User Feedback Simulation: Simulates the user providing feedback on the recommended destination.\n",
    "\n",
    "- Second Recommendation: After adjusting the weights based on feedback, the agent makes a new recommendation that reflects the updated reasoning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29fc98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended destination: Bangkok\n",
      "Feedback for Bangkok: Positive\n",
      "Updated weights: {'budget': 0.5238095238095238, 'luxury': 0.2857142857142857, 'adventure': 0.19047619047619047}\n",
      "\n",
      "Updated recommendation: Bangkok\n"
     ]
    }
   ],
   "source": [
    "# Simulate agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = ReflectiveTravelAgent()\n",
    "\n",
    "    # User's initial preferences\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.8,      # High preference for budget-friendly options\n",
    "        \"luxury\": 0.2,      # Low preference for luxury\n",
    "        \"adventure\": 0.5    # Moderate preference for adventure activities\n",
    "    }\n",
    "\n",
    "    # First recommendation based on initial preferences and weights\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Recommended destination: {recommended}\")\n",
    "\n",
    "    # Simulate user experience and provide feedback\n",
    "    agent.get_user_feedback(recommended)\n",
    "\n",
    "    # Second recommendation after adjusting weights based on feedback\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Updated recommendation: {recommended}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cb609",
   "metadata": {},
   "source": [
    "## Meta-reasoning with AI\n",
    "---\n",
    "\n",
    "Now let's bring in AI to perform meta-reasoning with agents. In this case we will use CrewAI framework to create our meta-reasoning Agents with OpenAI LLMs. We will also emulate a user feedback using AI just for demonstration purposes. First, let's make sure we initialize our OpenAI API key and then let's define the \"Crew\" (with CrewAI) and the Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff5a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "api_key = getpass.getpass(prompt=\"Enter OpenAI API Key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69487942",
   "metadata": {},
   "source": [
    "We will define three tools that our agents will use-\n",
    "\n",
    "1. `recommend_destination`: This tool will use a set of base weights that prioritizes budget, luxury, and adventure equally and then uses user's preference weights to recommend a destination. Paris will emphasize luxury, NYC emphasizes luxury and adventure, whereas Bangkok emphasizes budget.\n",
    "2. `update_weights_on_feedback`: This tool will update the internal base weights based on the user's feedback on the recommended destination. A positive feedback will tell the model that it's recommendation is correct and it needs to update it's internal base weights based and increase it by a given (arbitrary adjustment factor), or reduce the weights using the adjustment factor if the feedback is dissatisfied.\n",
    "3. `feedback_emulator`: This tool will emulate a user prividing \"satisfied\" or \"dissatisfied\" feedback to the AI agent's destination recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a18088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "\n",
    "@tool(\"Recommend travel destination based on preferences.\")\n",
    "def recommend_destination(user_preferences: dict) -> str:\n",
    "    \"\"\"\n",
    "    Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "    Args:\n",
    "        user_preferences (dict): User's preferences with keys - 'budget', 'luxury', 'adventure'\n",
    "                                default user_preference weights 'budget' = 0.8, 'luxury' = 0.2, 'adventure' = 0.5\n",
    "                                user_preferences = {\n",
    "                                                \"budget\": 0.8,\n",
    "                                                \"luxury\": 0.4,\n",
    "                                                \"adventure\": 0.3\n",
    "                                            }\n",
    "    Returns:\n",
    "        str: Recommended destination\n",
    "    \"\"\"\n",
    "    internal_default_weights = {\n",
    "            \"budget\": 0.33,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.33,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.33  # Weight for adventure-related preferences\n",
    "        }\n",
    "   # Calculate weighted scores for each destination\n",
    "    score = {\n",
    "        \"Paris\": (\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] +      # Paris emphasizes luxury\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"] +\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"]\n",
    "        ),\n",
    "        \"Bangkok\": (\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"] * 2 +  # Bangkok emphasizes budget\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"]\n",
    "        ),\n",
    "        \"New York\": (\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] * 1.5 +  # NYC emphasizes luxury and adventure\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"] * 1.5 +\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Select the destination with the highest calculated score\n",
    "    recommendation = max(score, key=score.get)\n",
    "    return recommendation\n",
    "\n",
    "@tool(\"Reasoning tool to adjust preference weights based on user feedback.\")\n",
    "def update_weights_on_feedback(destination: str, feedback: int, adjustment_factor: float) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze collected feedback and adjust internal preference weights based on user feedback for better future recommendations.\n",
    "\n",
    "    Args:        \n",
    "        destination (str): The destination recommended ('New York', 'Bangkok' or 'Paris')\n",
    "        feedback (int): Feedback score; 1 = Satisfied, -1 = dissatisfied\n",
    "        adjustment_factor (int): The adjustment factor between 0 and 1 that will be used to adjust the internal weights.\n",
    "                                 Value will be used as (1 - adjustment_factor) for dissatisfied feedback and (1 + adjustment_factor)\n",
    "                                 for satisfied feedback.\n",
    "    Returns:\n",
    "        dict: Adjusted internal weights\n",
    "    \"\"\"\n",
    "    internal_default_weights = {\n",
    "        \"budget\": 0.33,    # Weight for budget-related preferences\n",
    "        \"luxury\": 0.33,    # Weight for luxury-related preferences\n",
    "        \"adventure\": 0.33  # Weight for adventure-related preferences\n",
    "    }\n",
    "\n",
    "    # Define primary and secondary characteristics for each destination\n",
    "    destination_characteristics = {\n",
    "        \"Paris\": {\n",
    "            \"primary\": \"luxury\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        },\n",
    "        \"Bangkok\": {\n",
    "            \"primary\": \"budget\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        },\n",
    "        \"New York\": {\n",
    "            \"primary\": \"luxury\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Get the characteristics for the given destination\n",
    "    dest_chars = destination_characteristics.get(destination, {})\n",
    "    primary_feature = dest_chars.get(\"primary\")\n",
    "    secondary_feature = dest_chars.get(\"secondary\")\n",
    "\n",
    "    # adjustment_factor = 0.2  # How much to adjust weights by\n",
    "\n",
    "    if feedback == -1:  # Negative feedback\n",
    "        # Decrease weights for the destination's characteristics\n",
    "        if primary_feature:\n",
    "            internal_default_weights[primary_feature] *= (1 - adjustment_factor)\n",
    "        if secondary_feature:\n",
    "            internal_default_weights[secondary_feature] *= (1 - adjustment_factor/2)\n",
    "            \n",
    "    elif feedback == 1:  # Positive feedback\n",
    "        # Increase weights for the destination's characteristics\n",
    "        if primary_feature:\n",
    "            internal_default_weights[primary_feature] *= (1 + adjustment_factor)\n",
    "        if secondary_feature:\n",
    "            internal_default_weights[secondary_feature] *= (1 + adjustment_factor/2)\n",
    "\n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    total_weight = sum(internal_default_weights.values())\n",
    "    for key in internal_default_weights:\n",
    "        internal_default_weights[key] = round(internal_default_weights[key] / total_weight, 2)\n",
    "\n",
    "    # Ensure weights sum to exactly 1.0 after rounding\n",
    "    adjustment = 1.0 - sum(internal_default_weights.values())\n",
    "    if adjustment != 0:\n",
    "        # Add any rounding difference to the largest weight\n",
    "        max_key = max(internal_default_weights, key=internal_default_weights.get)\n",
    "        internal_default_weights[max_key] = round(internal_default_weights[max_key] + adjustment, 2)\n",
    "\n",
    "    return internal_default_weights\n",
    "\n",
    "@tool(\"User feedback emulator tool\")\n",
    "def feedback_emulator(destination: str) -> int:\n",
    "    \"\"\"\n",
    "    Given a destination recommendation (such as 'New York' or 'Bangkok') this tool will emulate to provide\n",
    "    a user feedback as 1 (satisfied) or -1 (dissatisfied)\n",
    "    \"\"\"\n",
    "    import random\n",
    "    feedback = random.choice([-1, 1])\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe973f1c",
   "metadata": {},
   "source": [
    "Once, the tools are defined, we will declare three CrewAI Agents each of which will use one of the tools above. The `meta_agent` is basically the agent that will perform meta-reasoning using the emulated user feedback and the previously recommended destination to update the internal weights using an `adjustment_factor`. \n",
    "\n",
    "Note that here, the model assigns an adjustment factor dynamically to adjust the internal system weights (which is `{\"budget\": 0.33, \"luxury\": 0.33, \"adventure\": 0.33}` in the beginning), i.e. we are not hard coding the adjustment factor. Although, the nature of user feedback in this example is limited to \"satisfied\" or \"dissatisfied\" (1 or -1), feedback can be of various forms and may contain more details, in which case your AI Agent may adjust different values to the adjustment_factor. More contextual feedback with details will help the model perform better meta-reasoning on it's previous responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e6b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel destination recommender\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUse the recommend_destination tool with these preferences: {'budget': 0.04, 'luxury': 0.02, 'adventure': 0.94}\n",
      "Return only the destination name as a simple string (Paris, Bangkok, or New York).\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel destination recommender\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to analyze the user's preferences which heavily favor adventure and very little for budget and luxury.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mRecommend travel destination based on preferences.\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"user_preferences\\\": {\\\"budget\\\": 0.04, \\\"luxury\\\": 0.02, \\\"adventure\\\": 0.94}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "New York\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel destination recommender\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "New York\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSimulated feedback provider\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUse the feedback_emulator tool with the destination from the previous task.\n",
      "Instructions:\n",
      "1. Get the destination string from the previous task\n",
      "2. Pass it directly to the feedback_emulator tool\n",
      "3. Return the feedback value (1 or -1)\n",
      "\n",
      "IMPORTANT: Pass the destination as a plain string, not a dictionary.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSimulated feedback provider\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mUser feedback emulator tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"destination\\\": \\\"New York\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "-1\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSimulated feedback provider\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "-1\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPreference weight adjuster\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUse the update_weights_on_feedback tool with:\n",
      "1. destination: Get from first task's output (context[0])\n",
      "2. feedback: Get from second task's output (context[1])\n",
      "3. adjustment_factor: a number betweek 0 and 1 that will be used to adjust internal weights based on feedback\n",
      "\n",
      "Ensure all inputs are in their correct types (string for destination, integer for feedback).\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPreference weight adjuster\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to adjust the preference weights based on the provided feedback for the destination 'New York', which received a dissatisfied feedback of -1. I will choose an adjustment factor between 0 and 1; for this case, I will use 0.1 for a slight adjustment.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mReasoning tool to adjust preference weights based on user feedback.\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"destination\\\": \\\"New York\\\", \\\"feedback\\\": -1, \\\"adjustment_factor\\\": 0.1}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'budget': 0.35, 'luxury': 0.32, 'adventure': 0.33}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPreference weight adjuster\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{'budget': 0.35, 'luxury': 0.32, 'adventure': 0.33}\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "Final Results: {'budget': 0.35, 'luxury': 0.32, 'adventure': 0.33}\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from typing import Dict, Union\n",
    "import random\n",
    "\n",
    "def generate_random_preferences():\n",
    "    # Generate 3 random numbers and normalize them\n",
    "    values = [random.random() for _ in range(3)]\n",
    "    total = sum(values)\n",
    "    \n",
    "    return {\n",
    "        \"budget\": round(values[0]/total, 2),\n",
    "        \"luxury\": round(values[1]/total, 2),\n",
    "        \"adventure\": round(values[2]/total, 2)\n",
    "    }\n",
    "\n",
    "# Initial shared state for weights, preferences, and results\n",
    "state = {\n",
    "    \"weights\": {\"budget\": 0.33, \"luxury\": 0.33, \"adventure\": 0.33},\n",
    "    \"preferences\": generate_random_preferences()\n",
    "}\n",
    "\n",
    "# Agents\n",
    "preference_agent = Agent(\n",
    "    name=\"Preference Agent\",\n",
    "    role=\"Travel destination recommender\",\n",
    "    goal=\"Provide the best travel destination based on user preferences and weights.\",\n",
    "    backstory=\"An AI travel expert adept at understanding user preferences.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[recommend_destination]\n",
    ")\n",
    "\n",
    "feedback_agent = Agent(\n",
    "    name=\"Feedback Agent\",\n",
    "    role=\"Simulated feedback provider\",\n",
    "    goal=\"Provide simulated feedback for the recommended travel destination.\",\n",
    "    backstory=\"An AI that mimics user satisfaction or dissatisfaction for travel recommendations.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[feedback_emulator]\n",
    ")\n",
    "\n",
    "meta_agent = Agent(\n",
    "    name=\"Meta-Reasoning Agent\",\n",
    "    role=\"Preference weight adjuster\",\n",
    "    goal=\"Reflect on feedback and adjust the preference weights to improve future recommendations.\",\n",
    "    backstory=\"An AI optimizer that learns from user experiences to fine-tune recommendation preferences.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[update_weights_on_feedback]\n",
    ")\n",
    "\n",
    "def process_recommendation_output(output: str) -> str:\n",
    "    \"\"\"Extract the clean destination string from the agent's output.\"\"\"\n",
    "    # Handle various ways the agent might format the destination\n",
    "    for city in [\"Paris\", \"Bangkok\", \"New York\"]:\n",
    "        if city.lower() in output.lower():\n",
    "            return city\n",
    "    return output.strip()\n",
    "\n",
    "def process_feedback_output(output: Union[Dict, str]) -> int:\n",
    "    \"\"\"Extract the feedback value from the agent's output.\"\"\"\n",
    "    if isinstance(output, dict):\n",
    "        return output.get('feedback', 0)\n",
    "    try:\n",
    "        # Try to parse as integer if it's a string\n",
    "        return int(output)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "# Tasks with data passing\n",
    "generate_recommendation = Task(\n",
    "    name=\"Generate Recommendation\",\n",
    "    agent=preference_agent,\n",
    "    description=(\n",
    "        f\"Use the recommend_destination tool with these preferences: {state['preferences']}\\n\"\n",
    "        \"Return only the destination name as a simple string (Paris, Bangkok, or New York).\"\n",
    "    ),\n",
    "    expected_output=\"A destination name as a string\",\n",
    "    output_handler=process_recommendation_output\n",
    ")\n",
    "\n",
    "simulate_feedback = Task(\n",
    "    name=\"Simulate User Feedback\",\n",
    "    agent=feedback_agent,\n",
    "    description=(\n",
    "        \"Use the feedback_emulator tool with the destination from the previous task.\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Get the destination string from the previous task\\n\"\n",
    "        \"2. Pass it directly to the feedback_emulator tool\\n\"\n",
    "        \"3. Return the feedback value (1 or -1)\\n\\n\"\n",
    "        \"IMPORTANT: Pass the destination as a plain string, not a dictionary.\"\n",
    "    ),\n",
    "    expected_output=\"An integer feedback value: 1 or -1\",\n",
    "    context=[generate_recommendation],\n",
    "    output_handler=process_feedback_output\n",
    ")\n",
    "\n",
    "adjust_weights = Task(\n",
    "    name=\"Adjust Weights Based on Feedback\",\n",
    "    agent=meta_agent,\n",
    "    description=(\n",
    "        \"Use the update_weights_on_feedback tool with:\\n\"\n",
    "        \"1. destination: Get from first task's output (context[0])\\n\"\n",
    "        \"2. feedback: Get from second task's output (context[1])\\n\"\n",
    "        \"3. adjustment_factor: a number betweek 0 and 1 that will be used to adjust internal weights based on feedback\\n\\n\"\n",
    "        \"Ensure all inputs are in their correct types (string for destination, integer for feedback).\"\n",
    "    ),\n",
    "    expected_output=\"Updated weights as a dictionary\",\n",
    "    context=[generate_recommendation, simulate_feedback]\n",
    ")\n",
    "\n",
    "# Crew Definition\n",
    "crew = Crew(\n",
    "    agents=[preference_agent, feedback_agent, meta_agent],\n",
    "    tasks=[generate_recommendation, simulate_feedback, adjust_weights],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "result = crew.kickoff()\n",
    "print(\"\\nFinal Results:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104c35a",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Self Explanation - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dd7e0",
   "metadata": {},
   "source": [
    "## ReflectiveTravelAgentWithSelfExplanation\n",
    "\n",
    "The `ReflectiveTravelAgentWithSelfExplanation` class simulates a travel agent that not only recommends destinations based on user preferences but also explains the reasoning behind its recommendations. \n",
    "\n",
    "1. **Preference-Based Recommendations**: It takes user preferences (like budget, luxury, and adventure preferences) and calculates scores for different travel destinations by weighing those preferences. The destination with the highest score is recommended to the user.\n",
    "\n",
    "2. **Self-Explanation**: For each recommendation, the agent generates a detailed self-explanation. This explanation outlines the factors that led to the recommendation, such as proximity to popular attractions, budget-friendly options, or the presence of adventure activities. The purpose is to provide transparency into how the decision was made, helping the user understand the reasoning process.\n",
    "\n",
    "3. **Feedback Reflection**: The agent doesn't stop after making the recommendation. It actively reflects on user feedback (whether positive or negative). If the feedback is negative, it introspects on its decision-making process and adjusts the importance (weights) it assigns to user preferences for future recommendations. For instance, if a user dislikes a budget-friendly recommendation, the agent might reduce the emphasis it places on budget-related preferences.\n",
    "\n",
    "4. **User Engagement**: The class also simulates a dialogue with the user. After giving the recommendation and the self-explanation, it collects feedback from the user, allowing for a more collaborative interaction. This feedback is then used to refine future recommendations, making the agent more adaptive and personalized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ebd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveTravelAgentWithSelfExplanation:\n",
    "    def __init__(self):\n",
    "        # Initialize the internal weights for user preferences (e.g., budget, luxury, adventure)\n",
    "        self.preferences_weights = {\n",
    "            \"budget\": 0.4,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.3,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.3  # Weight for adventure-related preferences\n",
    "        }\n",
    "\n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"\n",
    "        Recommend a destination based on user preferences and provide a self-explanation.\n",
    "\n",
    "        Args:\n",
    "            user_preferences (dict): User's preferences for different factors (e.g., budget, luxury, adventure)\n",
    "        \n",
    "        Returns:\n",
    "            (str, str): Recommended destination and the self-explanation\n",
    "        \"\"\"\n",
    "        # Score each destination by multiplying preference weights with user preferences\n",
    "        score = {\n",
    "            \"Paris\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] + \n",
    "                      self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"Bangkok\": (self.preferences_weights[\"budget\"] * user_preferences[\"budget\"] +\n",
    "                        self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"New York\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "                         self.preferences_weights[\"budget\"] * user_preferences[\"budget\"])\n",
    "        }\n",
    "        \n",
    "        # Choose the destination with the highest score\n",
    "        recommendation = max(score, key=score.get)\n",
    "        \n",
    "        # Generate and return a self-explanation for the recommendation\n",
    "        explanation = self.generate_self_explanation(recommendation, score[recommendation], user_preferences)\n",
    "        \n",
    "        return recommendation, explanation\n",
    "\n",
    "    def generate_self_explanation(self, destination, score, user_preferences):\n",
    "        \"\"\"\n",
    "        Generate a self-explanation for the recommended destination.\n",
    "        \n",
    "        Args:\n",
    "            destination (str): The recommended destination\n",
    "            score (float): The score assigned to the destination\n",
    "            user_preferences (dict): The user's preferences used for the recommendation\n",
    "        \n",
    "        Returns:\n",
    "            str: Self-explanation of the recommendation\n",
    "        \"\"\"\n",
    "        # Start the explanation with the destination and its score\n",
    "        explanation = (\n",
    "            f\"I recommended {destination} because it aligns with your preferences. \"\n",
    "            f\"The destination scored {score:.2f} based on the following factors:\\n\"\n",
    "        )\n",
    "        \n",
    "        # Customize the explanation for each destination based on user preferences\n",
    "        if destination == \"Paris\":\n",
    "            explanation += (\n",
    "                \"- High luxury offerings (aligned with your luxury preference).\\n\"\n",
    "                \"- Availability of adventure activities.\\n\"\n",
    "            )\n",
    "        elif destination == \"Bangkok\":\n",
    "            explanation += (\n",
    "                \"- Budget-friendly options (aligned with your budget preference).\\n\"\n",
    "                \"- Availability of adventure experiences.\\n\"\n",
    "            )\n",
    "        elif destination == \"New York\":\n",
    "            explanation += (\n",
    "                \"- Combination of luxury experiences and budget-friendly options.\\n\"\n",
    "            )\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "    def reflect_on_feedback(self, destination, user_feedback):\n",
    "        \"\"\"\n",
    "        Reflect on user feedback to improve decision-making in future recommendations.\n",
    "        \n",
    "        Args:\n",
    "            destination (str): The destination that was recommended\n",
    "            user_feedback (str): User feedback ('positive' or 'negative')\n",
    "        \"\"\"\n",
    "        # If the user provides negative feedback, adjust the internal reasoning process\n",
    "        if user_feedback == 'negative':\n",
    "            print(f\"User provided negative feedback for {destination}. Reflecting on reasoning...\")\n",
    "            \n",
    "            # Example: If Bangkok was chosen and the user disliked it, reduce budget weight\n",
    "            if destination == \"Bangkok\":\n",
    "                print(\"Realizing that budget weight might have been overemphasized. Reconsidering weights...\")\n",
    "                self.preferences_weights[\"budget\"] *= 0.9  # Reduce budget importance slightly\n",
    "\n",
    "            # If Paris, reduce importance of luxury if feedback is negative\n",
    "            elif destination == \"Paris\":\n",
    "                print(\"Luxury might have been over-prioritized. Adjusting luxury weight...\")\n",
    "                self.preferences_weights[\"luxury\"] *= 0.9\n",
    "\n",
    "            # Normalize weights after adjustment to maintain balance\n",
    "            total_weight = sum(self.preferences_weights.values())\n",
    "            for key in self.preferences_weights:\n",
    "                self.preferences_weights[key] /= total_weight  # Normalize weights\n",
    "\n",
    "            print(f\"Updated weights: {self.preferences_weights}\\n\")\n",
    "        else:\n",
    "            # Positive feedback indicates no changes are needed\n",
    "            print(f\"User provided positive feedback for {destination}. No changes needed.\")\n",
    "\n",
    "    def engage_with_user(self, recommendation, explanation):\n",
    "        \"\"\"\n",
    "        Simulate user interaction by providing a self-explanation and inviting feedback.\n",
    "\n",
    "        Args:\n",
    "            recommendation (str): The recommended destination\n",
    "            explanation (str): Self-explanation for the recommendation\n",
    "        \"\"\"\n",
    "        # Show the recommendation and its explanation to the user\n",
    "        print(f\"Recommended destination: {recommendation}\")\n",
    "        print(f\"Self-explanation: {explanation}\")\n",
    "\n",
    "        # Simulate user feedback (positive or negative)\n",
    "        user_feedback = input(f\"Did you like the recommendation for {recommendation}? (positive/negative): \")\n",
    "        \n",
    "        # Reflect on feedback and adjust the agent's reasoning process if needed\n",
    "        self.reflect_on_feedback(recommendation, user_feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb70b41",
   "metadata": {},
   "source": [
    "The provided code below simulates the usage of the `ReflectiveTravelAgentWithSelfExplanation` class, showcasing how the travel agent interacts with the user. \n",
    "\n",
    "1. **Agent Creation**: It first creates an instance of the `ReflectiveTravelAgentWithSelfExplanation` class, which initializes the agent with predefined weights for user preferences like budget, luxury, and adventure.\n",
    "\n",
    "2. **User Preferences**: It defines a sample user's travel preferences. In this case, the user has a high preference for budget-friendly options (`budget: 0.7`), a low preference for luxury experiences (`luxury: 0.2`), and a moderate preference for adventure activities (`adventure: 0.6`).\n",
    "\n",
    "3. **Generate Recommendation and Self-Explanation**: The agent uses the `recommend_destination` method to recommend a travel destination based on these preferences. Along with the recommendation, the agent also generates a self-explanation, which describes why the particular destination was chosen.\n",
    "\n",
    "4. **User Engagement and Feedback**: The agent then engages with the user by displaying the recommended destination and its explanation. Afterward, it collects feedback from the user (whether they liked the recommendation or not) and uses that feedback to reflect on its decision-making process, adjusting its internal reasoning if necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb5337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended destination: Bangkok\n",
      "Self-explanation: I recommended Bangkok because it aligns with your preferences. The destination scored 0.46 based on the following factors:\n",
      "- Budget-friendly options (aligned with your budget preference).\n",
      "- Availability of adventure experiences.\n",
      "\n",
      "Did you like the recommendation for Bangkok? (positive/negative): negative\n",
      "User provided negative feedback for Bangkok. Reflecting on reasoning...\n",
      "Realizing that budget weight might have been overemphasized. Reconsidering weights...\n",
      "Updated weights: {'budget': 0.37500000000000006, 'luxury': 0.3125, 'adventure': 0.3125}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Simulating agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the reflective travel agent with self-explanation\n",
    "    agent = ReflectiveTravelAgentWithSelfExplanation()\n",
    "    \n",
    "    # Example: User's preferences (high budget preference, low luxury, moderate adventure)\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.7,      # High preference for budget-friendly options\n",
    "        \"luxury\": 0.2,      # Low preference for luxury\n",
    "        \"adventure\": 0.6    # Moderate preference for adventure activities\n",
    "    }\n",
    "    \n",
    "    # Get the destination recommendation and self-explanation\n",
    "    recommendation, explanation = agent.recommend_destination(user_preferences)\n",
    "    \n",
    "    # Engage with the user by providing the recommendation, explanation, and receiving feedback\n",
    "    agent.engage_with_user(recommendation, explanation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f01747",
   "metadata": {},
   "source": [
    "# Self Modeling - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e14cd4",
   "metadata": {},
   "source": [
    "The `ReflectiveTravelAgentWithSelfModeling` class represents a sophisticated travel recommendation system that utilizes **self-modeling** to enhance its decision-making and adaptability. \n",
    "\n",
    "### 1. **Initialization:**\n",
    "   - **Self-Model and Knowledge Base:** The agent starts with an internal model that includes its goals and a knowledge base. \n",
    "     - **Goals:** Initially, the goals are set to provide personalized recommendations, optimize user satisfaction, and not prioritize eco-friendly options by default.\n",
    "     - **Knowledge Base:** It contains information about various travel destinations, including their ratings, costs, luxury levels, and sustainability. This base also tracks user preferences.\n",
    "\n",
    "### 2. **Updating Goals:**\n",
    "   - **Adapting to Preferences:** When new user preferences are provided, the agent can update its goals accordingly. For example, if the user prefers eco-friendly options, the agent will adjust its goals to prioritize recommending sustainable travel options. Similarly, if the user’s budget changes, the agent will refocus on cost-effective recommendations.\n",
    "\n",
    "### 3. **Updating Knowledge Base:**\n",
    "   - **Incorporating Feedback:** After receiving feedback from users, the agent updates its knowledge base. If the feedback is positive, the agent increases the rating of the recommended destination. If the feedback is negative, the rating is decreased. This helps the agent refine its recommendations based on real user experiences.\n",
    "\n",
    "### 4. **Making Recommendations:**\n",
    "   - **Calculating Scores:** The agent evaluates each destination by calculating a score based on its rating and, if eco-friendly options are a goal, it adjusts the score by adding the sustainability rating.\n",
    "   - **Selecting the Best Destination:** The destination with the highest score is recommended to the user. This process ensures that the recommendation aligns with both user preferences and the agent’s goals.\n",
    "\n",
    "### 5. **Engaging with the User:**\n",
    "   - **Providing Recommendations:** The agent presents the recommended destination to the user and asks for feedback.\n",
    "   - **Feedback Handling:** The feedback (positive or negative) is used to update the knowledge base, which helps improve future recommendations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveTravelAgentWithSelfModeling:\n",
    "    def __init__(self):\n",
    "        # Initialize the agent with a self-model that includes goals and a knowledge base\n",
    "        self.self_model = {\n",
    "            \"goals\": {\n",
    "                \"personalized_recommendations\": True,\n",
    "                \"optimize_user_satisfaction\": True,\n",
    "                \"eco_friendly_options\": False  # Default: Not prioritizing eco-friendly options\n",
    "            },\n",
    "            \"knowledge_base\": {\n",
    "                \"destinations\": {\n",
    "                    \"Paris\": {\"rating\": 4.8, \"cost\": 2000, \"luxury\": 0.9, \"sustainability\": 0.3},\n",
    "                    \"Bangkok\": {\"rating\": 4.5, \"cost\": 1500, \"luxury\": 0.7, \"sustainability\": 0.6},\n",
    "                    \"Barcelona\": {\"rating\": 4.7, \"cost\": 1800, \"luxury\": 0.8, \"sustainability\": 0.7}\n",
    "                },\n",
    "                \"user_preferences\": {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def update_goals(self, new_preferences):\n",
    "        \"\"\"Update the agent's goals based on new user preferences.\"\"\"\n",
    "        if new_preferences.get(\"eco_friendly\"):\n",
    "            self.self_model[\"goals\"][\"eco_friendly_options\"] = True\n",
    "            print(\"Updated goal: Prioritize eco-friendly travel options.\")\n",
    "        if new_preferences.get(\"adjust_budget\"):\n",
    "            print(\"Updated goal: Adjust travel options based on new budget constraints.\")\n",
    "    \n",
    "    def update_knowledge_base(self, feedback):\n",
    "        \"\"\"Update the agent's knowledge base based on user feedback.\"\"\"\n",
    "        destination = feedback[\"destination\"]\n",
    "        if feedback[\"positive\"]:\n",
    "            # Increase rating for positive feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] += 0.1\n",
    "            print(f\"Positive feedback received for {destination}; rating increased.\")\n",
    "        else:\n",
    "            # Decrease rating for negative feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] -= 0.2\n",
    "            print(f\"Negative feedback received for {destination}; rating decreased.\")\n",
    "    \n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"Recommend a destination based on user preferences and the agent's self-model.\"\"\"\n",
    "        # Store user preferences in the agent's self-model\n",
    "        self.self_model[\"knowledge_base\"][\"user_preferences\"] = user_preferences\n",
    "        \n",
    "        # Update agent's goals based on new preferences\n",
    "        if user_preferences.get(\"eco_friendly\"):\n",
    "            self.update_goals(user_preferences)\n",
    "        \n",
    "        # Calculate scores for each destination\n",
    "        best_destination = None\n",
    "        highest_score = 0\n",
    "        for destination, info in self.self_model[\"knowledge_base\"][\"destinations\"].items():\n",
    "            score = info[\"rating\"]\n",
    "            if self.self_model[\"goals\"][\"eco_friendly_options\"]:\n",
    "                # Boost score for eco-friendly options if that goal is prioritized\n",
    "                score += info[\"sustainability\"]\n",
    "            \n",
    "            # Update the best destination if current score is higher\n",
    "            if score > highest_score:\n",
    "                best_destination = destination\n",
    "                highest_score = score\n",
    "        \n",
    "        return best_destination\n",
    "\n",
    "    def engage_with_user(self, destination):\n",
    "        \"\"\"Simulate user engagement by providing the recommendation and receiving feedback.\"\"\"\n",
    "        print(f\"Recommended destination: {destination}\")\n",
    "        # Simulate receiving user feedback (e.g., through input in a real application)\n",
    "        feedback = input(f\"Did you like the recommendation of {destination}? (yes/no): \").strip().lower()\n",
    "        positive_feedback = feedback == \"yes\"\n",
    "        return {\"destination\": destination, \"positive\": positive_feedback}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6fdea",
   "metadata": {},
   "source": [
    "The provided code snippet is designed to simulate the usage of the `ReflectiveTravelAgentWithSelfModeling` class. \n",
    "\n",
    "### 1. **Creating an Instance of the Agent:**\n",
    "   ```python\n",
    "   agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "   ```\n",
    "   - **Purpose:** Initializes a new instance of the `ReflectiveTravelAgentWithSelfModeling` class.\n",
    "   - **Outcome:** This instance represents a travel agent equipped with self-modeling capabilities, including goal management and a knowledge base.\n",
    "\n",
    "### 2. **Setting User Preferences:**\n",
    "   ```python\n",
    "   user_preferences = {\n",
    "       \"budget\": 0.6,            # Moderate budget constraint\n",
    "       \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "       \"adventure\": 0.7,         # High preference for adventure\n",
    "       \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "   }\n",
    "   ```\n",
    "   - **Purpose:** Defines a set of preferences provided by the user.\n",
    "   - **Outcome:** These preferences indicate that the user has a moderate budget, moderate luxury preferences, a high interest in adventure, and a strong preference for eco-friendly options.\n",
    "\n",
    "### 3. **Getting a Recommendation:**\n",
    "   ```python\n",
    "   recommendation = agent.recommend_destination(user_preferences)\n",
    "   ```\n",
    "   - **Purpose:** Requests a travel destination recommendation from the agent based on the provided user preferences.\n",
    "   - **Outcome:** The agent processes the preferences, updates its goals if necessary (e.g., prioritizing eco-friendly options), and selects the best destination to recommend.\n",
    "\n",
    "### 4. **Engaging with the User:**\n",
    "   ```python\n",
    "   feedback = agent.engage_with_user(recommendation)\n",
    "   ```\n",
    "   - **Purpose:** Simulates interaction with the user by presenting the recommendation and gathering feedback.\n",
    "   - **Outcome:** The user provides feedback on the recommended destination, which is used to evaluate the effectiveness of the recommendation.\n",
    "\n",
    "### 5. **Updating the Knowledge Base:**\n",
    "   ```python\n",
    "   agent.update_knowledge_base(feedback)\n",
    "   ```\n",
    "   - **Purpose:** Updates the agent’s knowledge base with the feedback received from the user.\n",
    "   - **Outcome:** The agent adjusts its knowledge base by modifying ratings or other attributes based on whether the feedback was positive or negative. This update helps improve future recommendations by refining the agent's understanding of user preferences and destination qualities.\n",
    "\n",
    "### Summary:\n",
    "In essence, this code snippet demonstrates how the `ReflectiveTravelAgentWithSelfModeling` class operates in a simulated environment. It initializes the agent, sets user preferences, obtains a recommendation, engages the user for feedback, and updates the agent’s knowledge base based on that feedback. This simulation helps illustrate the agent’s self-modeling capabilities and its ability to adapt and improve recommendations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated goal: Prioritize eco-friendly travel options.\n",
      "Recommended destination: Barcelona\n",
      "Did you like the recommendation of Barcelona? (yes/no): no\n",
      "Negative feedback received for Barcelona; rating decreased.\n"
     ]
    }
   ],
   "source": [
    "# Simulating agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the reflective travel agent with self-modeling\n",
    "    agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "    \n",
    "    # Example user preferences including a focus on eco-friendly options\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.6,            # Moderate budget constraint\n",
    "        \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "        \"adventure\": 0.7,         # High preference for adventure\n",
    "        \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "    }\n",
    "    \n",
    "    # Get the recommended destination based on user preferences\n",
    "    recommendation = agent.recommend_destination(user_preferences)\n",
    "    \n",
    "    # Engage with the user to provide feedback on the recommendation\n",
    "    feedback = agent.engage_with_user(recommendation)\n",
    "    \n",
    "    # Update the knowledge base with the user feedback\n",
    "    agent.update_knowledge_base(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3c2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
